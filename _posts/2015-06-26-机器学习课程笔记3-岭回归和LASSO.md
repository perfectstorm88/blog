---
layout: post
categories: 机器学习 人工智能
---

## 1.大纲
- 扔给你一堆数据，可能有成百上千各变量,怎样变量选择，怎样排除多重共线性问题：
    - 要么丰厚的业务知识丰富，根据经验来选择；
    - 要么让数据自己说话，把相关的变量筛选出来，这是很典型的数据挖掘
- 介绍两个回归方法
    - 岭回归(Ridge Regression,RR)
    - LASSO(The Least Absolute Shrinkage and Selectionator operator)，读[赖搜]，最小的绝对值收缩和变量选择方法

## 2.多元线性回归的最小二乘解
`多元线性回归的最小二乘解`，可以简化公式，简化数学模型，加快对问题的理解速度
多元线性模型可表示为

- Y = Xβ+e     #(注意β0表示截距项，截距项系数为1)

类似于一元线性回归，求参数β的估计值，就是求最小二乘函数达到最小的β值

- Q(β)=(y-Xβ)<sup>T</sup>(y-Xβ)         #转置相乘，就是截距平方

可以证明β的最小二乘估计（求偏导数或几何解释）

- β=(X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>y        #(转置，求逆，(X<sup>T</sup>X)<sup>-1</sup>叫做广义逆)

如果X是个方阵，抵消后

- β=X<sup>-1<sup>y

### 2.1.几何解释
一幅图画一下，让大家理解回归在几何上是什么意义
由于多了截距项，用几何解释的难度高了10倍以上，所以本课程的例子都是对变量进行了中心化、标准化处理，要把截距项去掉。
如y~(x1,x2)，y=β1X1+β2X2 (β1,β2是未知数)
y-β1X1-β2X2 就是残差。再平方就是残差平方。
__回归就是几何上求最短距离的问题__

### 2.2.广义逆的奇异性

- β=X<sup>+</sup>Y=(X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>Y     （X<sup>+</sup>表示X的广义逆或伪逆）
    - 当变量比样本多，出现奇异性
    - 当出现多重共线性，出现奇异性

注意，不是所有的矩阵都存在广义逆的，如果存在多重共线性，就不是满秩的
当然对于线性回归的抽样来说，直接线性相关的可能性太低了，存在误差项，加加减减直接为零不大可能，
可能元素很小，零点零零几这么分量，在这种请求依然可以求逆，但是非常不稳定，可能求出的变量系数非常庞大。结果不稳定，求出得解没有意义。

### 2.3.无偏估计
β表示期望值， β弯根据样本估算出值。进行成千上万次抽样，计算出多个β弯，再取平均值，无限逼近真实值
后面讲的岭回归和LASSO都是有偏估计

## 3.岭回归
为什么叫岭回归，要画一张岭迹图(跟山岭的形状差不多)，通过目测，好像看风水一样，估计一个参数

- β(k)=(X<sup>T</sup>X+λI)<sup>-1</sup>X<sup>T</sup>y   # λI，λ乘以单位阵作为扰动，这样就可以求逆

等价模型：最小二乘函数 + 惩罚函数
<img src="http://img.lichangzhen.top/ml岭回归公式.jpg" width = "300" height = "200" alt="图片名称" align=center />
后面部分叫做惩罚函数，目的是不能让β值变的很大

### 3.1.几何意义
二维为例，RSS是个曲面。β1，β2的平方是个圆，
有一把刀，切过来，曲面的切面，与圆方的交点，就是岭回归估计：

- 当岭参数λ=0,等同于最小二乘解
- 当λ趋向于更大时，岭回归系数估计趋向于0
### 3.2.岭回归的性质
__1. 有偏估计__
有偏估计，先天不足，只是为了解决多重共线性的权宜之计；通常会使残差平方和变大，但系数拟合更好，
__2. β弯是β的一个线性变换__

- β弯(k)=(X<sup>T</sup>X+λI)<sup>-1</sup>X<sup>T</sup>y=(X<sup>T</sup>X+λI)<sup>-1</sup> X<sup>T</sup>X (X<sup>T</sup>X)<sup>-1</sup> X<sup>T</sup>y=(X<sup>T</sup>X+λI)<sup>-1</sup>X<sup>T</sup>X β弯

在实际应用中，由于岭参数k总是通过数据来确定，因而k也依赖于y，从本质上说β弯(k)并非β弯的线性变换，也不是y的线性函数
__3. 当成向量的话，β弯(k)比β弯短__
当作向原点的压缩
__4. 找到一个k,比最小二乘法更能逼近理想β【非常重要】__

### 3.3.λ的选择原则
- 各回归系数的岭估计基本稳定（不能正负穿梭）
- 残差平方和(略增大)增大不太多
- 其它。。。

### 3.4.用岭回归选择变量
- 中心化标准化后，可以剔除系数很小
- 随着增大，不稳定(正负穿梭)，震动趋于零的变量可以剔除
- 去掉变量后，可以多重复几次
- **比较适合处理多重共线性问题，不适合做变量筛选**

### 3.5.岭回归存在的问题
方法太多，差异太大
岭迹图，直观性太强，随意性太大，流程性不强
不能做变量筛选，岭回归返回的模型（自己不筛选变量），包含了所有的变量。

### 3.6.通过R演练
```r
library(MASS)
longley
summary(fml<- lm(Employed~.,data=longley)) #相关系数平方很高，但是系数检验不大理想
#岭回归
#names(longley)[1]<-"y" #对第一列进行重命名
lm.ridge(GNP.deflator~.,longley) #没有指出λ,缺省是做线性回归
plot(lm.ridge(GNP.deflator~.,longley,lambda=seq(0,0.1,0.001))) #指出λ从0到0.1，步长0.001
#λ趋向零的时候，趋于不稳定，所以断定有多重共线性
select(lm.ridge(GNP.deflator~.,longley,lambda=seq(0,0.1,0.001))) #怎么取岭参数λ，通常取GCV或者三者投票

install.packages("ridge") #一个叫扩展包实现了岭回归
library(ridge) #
summary(a<-linearRidge(GNP.deflator~.,data=longley));#自动取了岭参数，
```
目前很少在数据挖掘中看到岭回归，看风水，有些心惊肉跳，不大踏实
## 4.LASSO
lasso这个单词,套索，牛仔用的
通过构建一阶惩罚函数获得一个精炼模型，最终确定一些变量的系数为零（岭回归接近零）
擅长多重共线性和变量筛选，有偏估计。

### 4.1.LASSO vs 岭回归
LASSO_vs_岭回归_函数
![](http://img.lichangzhen.top/mlLASSO_vs_岭回归_函数.jpg)
LASSO_vs_岭回归_几何含义
![](http://img.lichangzhen.top/mlLASSO_vs_岭回归_几何含义.jpg)
LASSO_vs_岭回归_变量系数
![](http://img.lichangzhen.top/mlLASSO_vs_岭回归_变量系数.jpg)

参考《The Elements of Statistical Learning》统计学习的要素，统计学的方法应用到机器学习中

#### 4.1.1.弹性网
Elastic Net:目前最好的一种收缩方法，无论多重共线性还是筛选变量

- λΣ(αβ<sub>j</sub><sup>2</sup>+(1-α)|β<sub>j</sub>|)

### 4.2.求解过程LAR算法
存在尖角，这个地方不可导的，所以求偏导方法不可行：
- 向前逐步回归(Forward Stepwise)
- 2004年，非常聪明的一个人Efron提出，**使用LAR**，Least Angel Regression最小角回归

最小角回归
1.选择一个变量，相关性最高，顺着一条路径，进行游走，
当前进的时候，走的是角平分线，
76页，在图上是什么意义？最终最小二乘解为零，与所有变量的夹角垂直？

L<sub>1</sub> arc Length  弧长
修正的LAR
如果有各个非零的系数从零又变成零，这个系数可以丢弃掉

```r
install.packages("lars") #http://cran.rstudio.com/ ->TASK Views->Machine Learning-> search lars
library(lars) #
w=as.matrix(longley)
laa=lars(w[,2:7],w[,1])
plot(laa)
summary(laa)  #CP指标，越小越好

summary(fml<- lm(GNP.deflator~GNP+Unemployed+Population,data=longley))
```
