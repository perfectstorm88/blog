---
layout: post
categories: 机器学习 人工智能
---


- 一元以及多元的线性回归，统计学东西比较多，统计学术语一堆堆的
- 《大数据的统计学基础》基础的基础
- logistic回归，划入广义线性回归模型。
- 变量筛选，从一堆变量中求解出来，以及降维。

## 1.归纳
**拟合**，一般选择直线或者次数比较低得曲线。（测试有误差，曲线如果经过每一个点，叫拟合过度。虽然模型内的非常准确，但是进行预测则可能更不准确）
学习集，预测。回归模型，w=a+bh

- 线性回归：
    - 一元线性(自变量一个)，
    - 多元线性（自变量多个，一次方程，是个曲面，高维空间中的超平面）；
- 非线性回归：二次，logistic等。非线性用线性表示,叫广义线性（如logistic）
- 困难：选定变量(多元)，降维是回归模型中的难点。世界规律都是用很简单的东西，
**多重共线性** （有些变量是打酱油的，怎么判断，怎么去掉）
怎样检验模型是否合理，需要一些检验手段。

## 2.回归就是变量之间的关系
- 自变量和因变量的关系
- 函数关系：确定性，y=a+bx（a截距项、b斜率）
- 相关性：非确定性关系

## 3.相关系数
决定是否适合去做回归模型，相关系数去衡量线性相关性的强弱。
公式中的几个概念：

- 下标，表示第几个样本。
- x拔(求平均值) 
- 西格玛(求和)如果不带上下标表示全部求和。
- 根据柯西不等式，都小于1.如果接近1，适合用线性回归模型
- 正相关系数，同增。负数，表示~~

## 4.RSS
- 哪个回归线效果最好呢：比较直观的做法，点到直线的距离，使得所有点得距离之和最小。
- 但是麻烦是，距离涉及到开方，很难转换为极值。就改为垂直线，or 平行于y轴，称为残差
- 绝对值在数学里不好求极值，所以改为求平方
- RSS: residual sum of squares, 剩余/误差/残差平方和，衡量预测值不真实值的差距
- RSS(最小二乘法)，二次函数求极值的方法。
- 如何求极值：求偏导数，有两个自变量，就需要求两个偏导数，然后解二元方程组。

## 5.线性回归通过R语言
- y=c(61,57,58,40,90,35,68)
- x=c(170,168,175,153,185,135,172)
- plot(x,y) #把散点画出来
- z=lm(y~x+1) #lm 假定 y=ax+b, 后面的+1可以不写
- z=lm(y~x-1) # **过原点，没有截距**
- plot(y~x+1) 
- summary(z) #求解，summary中各个字段含义：
    - (Intercept)  截距
    - residual 残差、
    - Residual standard 残差的标准差
    - Multiple R-squared: 相关系数平方，越高表示相关性越好
    - Adjusted R-squared:调整后的拟合优度，作用有限
    - t value假设检验的统计量t值，
    - Pr(>|t|)  t以外的面积有多大，这个值越小越好。
    - F-statistic: F统计
    - p-value  整体的假设检验。不能说我有错。假设不对，回归模型是无效的

- plot(z) 要把图拉大些，有多个图，要按多个回车
- deviance(z) 误差平方和
- residuals(z) 计算残差
- print(z) 打印模型信息
- anova(z) 方法分析表

```r
Analysis of Variance Table

Response: y
          Df  Sum Sq Mean Sq F value   Pr(>F)   
x          1 197.633 197.633  47.943 0.006176 **
Residuals  3  12.367   4.122                    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```

预测,其中x是自变量，m是要预测的值，z是公式
m=data.frame(x=185) 
predict(z,m)
## 6.多元线性模型
R语言中输入swiss，内置了swisss数据集

```r
 swiss.lm=lm(Fertility~.,data=swiss)
 summary(swiss.lm)
```
数据太多时，残差用四分数表示：Min 1Q media 3Q Max

## 7.虚拟变量/哑变量
哑变量\虚拟变量，如sex这个分类变量用两个哑变量表示：isman,iswoman

- 加法模型，哑变量用来调整截距：,如w=a+bh+c*isman
- 乘法模型，哑变量用来调整斜率, 如w=a+bh+c*isman*h  
- 混合模型，即影响截距和斜率上，w=a+bh+c*isman+d*isWoman+e*isman*h+f*isWoman*h+g

## 8.多元线性回归模型

模型修正,参见R-Modeling 324页

- lm.new<-update(lm.sol, .~.+I(X2^2)) #I(X2^2) 表示X2的平方项
- lm2.new<-update(lm.new, .~.-X2) #去掉X2的一次项 
- lm3.new<-update(lm.new, .~.+X1*X2) #增加考虑X1和X2的一次项 

__说明：__这个修正都是靠着分析师的经验和肉眼观察，碰出来的。统计学上有没有机械化的，支持变量选择的方法呢，这个有---逐步回归。有下面几种种：

- 向前引入法：从一元回归开始，逐步增加变量
- 向后剔除法：所有变量，逐步剔除
- 逐步筛选法：结合上述两种

评定方法：

- RSS(残差平方和),对应summary结果的Residual standard error
- R^2(相关系数平方),对应summary结果的Multiple R-squared
- AIC (Akaike information criterion)赤池信息准则

```r
s=lm(Fertility~.,data=swiss)
s1=step(s,direction="forward") #已经没有变量可以增加了
s1=step(s,direction="backward")
s1=step(s,direction="both")
```

手工回归,R-Modeling 334页

- add1()
- drop1()

## 9.回归诊断
- 样本是否符合正态分布？
    - 正态性检验:函数shapiro.test( x$x1)
    - P>0.05,正态性分布
- 学习集/是否存在离群值?怎么发现离群值
- 线性模型是否合理？可能自然界的关系更复杂
- 误差是否满足独立性、等方差（误差与y大小没有关系）
    - 如果样本是正态分布的，残差residuals()也是正态分布的
- 多重共线性（自变量不是独立的）
    - 多重共线性存在，会导致求逆矩阵的结果非常不确定
    - Kappa值，希腊字母，把样本的数据乘以它的矩阵的转置，在求特征根，最大值除以最小值
    - k<100,说明共线性程度小,如果100< k< 1000,有较强的多重共线性,k>1000,在严重的多重共线

## 10.广义线性模型
- 非线性
- S型曲线，统计学非常有名，叫logistic曲线
- glm()  拟合广义线性模型(Fitting Generalized Linear Models)

下面是Norell实验：

```r
norell<-data.frame(x=0:5, n=rep(70,6),success=c(0,9,21,47,60,63))
norell$Ymat<- cbind(norell$success, norell$n-norell$success)
glm.sol<-glm(Ymat~x, family=binomial, data=norell)
summary(glm.sol)
```

__广义线性模型转换为线性的方法__

- 对数法,y=a+b logx，lm.log=lm(y~log(x))
- 指数法,y=a ebx，lm.exp=lm(log(y)~x)
- 幂函数法,y=a xb，lm.pow=lm(log(y)~log(x))