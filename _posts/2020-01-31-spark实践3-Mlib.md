---
layout: post
categories: 大数据 spark
---

# 官方理解要点

- 分类、回归、聚类算法的df默认字段 features、label、prediction
  - features为Vector
- 线性代数，
  - dense vectors等同于NumPy array
  - sparse vectors 可以通过SciPy scipy.sparse column vectors  构建 SparseVector
    - SparseVector第一个参数是size，后面的是数组或者字典
- 推荐模型pyspark.ml.recommendation.ALS  交替最小二乘法矩阵分解，默认字段
  - userCol='user', itemCol='item', seed=None, ratingCol='rating'
- 关联规则模型：pyspark.ml.fpm.FPGrowth
  -  itemsCol='items', predictionCol='prediction'


## 神奇的StringIndexer和IndexToString，IndexToString是如何自动找到转换元数据的？？很神奇
- index_to_string_example.py

找到原因了，通过labels参数进行反向映射，IndexToString(inputCol="prediction", outputCol="prediction_str",labels=label_model.labels)
## 变量重要性
[Feature Selection Using Feature Importance Score - Creating a PySpark Estimator](https://www.timlrx.com/2018/06/19/feature-selection-using-feature-importance-score-creating-a-pyspark-estimator/)

- 并非所有的分类算法都有featureImportances, 随机森林、决策树等有变量重要性
- model.featureImportances中为变量重要性,是个Sparse Vector,样例如下：
```
(5,[0,1,2,3,4],[0.21051712326512742,0.23106698371089118,0.02428571428571427,0.10856479142520334,0.42556538731306376])
```
- 写一个转换函数，使用features的元数据，通过idx找到对应得分，可以转换出最终结果
```python
# 
print('计算变量重要性结果', model)
if not hasattr(model, 'featureImportances'):
    print(f'{model.__class__}没有变量重要性指标')
    return {}
def ExtractFeatureImp(featureImp, dataset, featuresCol):
  list_extract = []
  attrs = dataset.schema[featuresCol].metadata["ml_attr"]["attrs"]
  for i in attrs:
      list_extract = list_extract + attrs[i]
  varlist = pd.DataFrame(list_extract)
  varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])
  return(varlist.sort_values('score', ascending = False))
x = ExtractFeatureImp(model.featureImportances, dataset, "features").head(10)
```
最终转换后的结果如下：
```
   idx                        name     score
0    4                        id_0  0.328545
1    0            text_spark f g h  0.284976
4    3  text_a b c d e spark a a a  0.177367
2    1                    text_b d  0.151351
3    2       text_hadoop mapreduce  0.057761
```

# 参考

- [pyspark.ml package](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html)
- [矩阵分解模型（1）：ALS学习算法](https://blog.csdn.net/oucpowerman/article/details/49847979)
