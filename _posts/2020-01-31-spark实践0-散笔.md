---
layout: post
categories: 大数据 spark
---

Spark散笔，未汇总

[spark核心技术原理透视一（Spark运行原理）](https://blog.csdn.net/liuxiangke0210/article/details/79687240) 
这篇文章不错的，spark原理可以了解下

[如何理解spark中RDD和DataFrame的结构？](https://www.zhihu.com/question/48684460)

[RDD、DataFrame和DataSet的区别](https://www.jianshu.com/p/c0181667daa0)
*     RDD不了解内部结构，
* RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等
* RDD API是函数式的，强调不变性，在大部分场景下倾向于创建新对象而不是修改老对象
* Spark SQL在框架内部已经在各种可能的情况下尽量重用对象，这样做虽然在内部会打破了不变性，但在将数据返回给用户时，还会重新转为不可变数据
* Spark SQL还可以根据数据文件中附带的统计信息来进行剪枝。简单来说，在这类数据格式中，数据是分段保存的，每段数据都带有最大值、最小值、null值数量等
* Spark SQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。
* Dataset可以认为是DataFrame的一个特例，主要区别是Dataset每一个record存储的是一个强类型值而不是一个Row。后面版本DataFrame会继承DataSet，DataFrame是面向Spark SQL的接口
* 从Spark 2.0开始，Dataset开始具有两种不同类型的API特征：有明确类型的API和无类型的API
    * 
* 该选用RDD、DataFrame或Dataset看起来好像挺明显。前者可以提供底层的功能和控制，后者支持定制的视图和结构，可以提供高级和特定领域的操作，节约空间并快速运行
* 当我们回顾从早期版本的Spark中获得的经验教训时，我们问自己该如何为开发者简化Spark呢？该如何优化它，让它性能更高呢？我们决定把底层的RDD API进行高级抽象，成为DataFrame和Dataset，用它们在Catalyst优化器和Tungsten之上构建跨库的一致数据抽象

[机器学习实战](https://github.com/frankstar007/kNN)


[Spark编程笔记(8)-Spark MLlib](https://www.zhihu.com/search?q=Mlib&type=content&range=3m)  作者是蚂蚁金服的，写的还不错的，推荐一读

* Spark作为现阶段Hadoop架构中，替代Map-Reduce进行分布式处理的主要数据处理组件，和HDFS、Hbase等数据组件共同构成了现阶段主流大数据工具系统。现阶段已经不需要通过抽样方法来实现建模操作。
* 不进行抽样，而是使用全量样本进行建模分析，是大数据建模技术的标志。
* Pyspark的即席查询也是一个关键
* spark-shell  查看版本号


# pipeline管道理解
[Pipeline详解及Spark MLlib使用](https://zhuanlan.zhihu.com/p/24311565):其实对应[ML Pipelines](https://spark.apache.org/docs/latest/ml-pipeline.html)，翻译过来

- 如一个文档数据的处理工作流可能包括下列步骤,如分词，向量化，训练，将上述的工作流描述为管道，它包含一系列需要被执行的有顺序的管道阶段（转换器和估计器）
- 每个转换器或估计器实例有唯一的编号，这个特征在制定参数的时候非常有用
- 如果是估计器，管道先调用该估计器的fit()方法来产生模型。如果管道还有其它更多阶段，在将数据框传入下一个阶段之前，管道会先调用模型的transform()方法
- 也可以产生非线性的管道，数据流向为无向非环图(DAG)。这种图通常需要明确地指定每个阶段的输入和输出列名（通常以指定参数的形式）。如果管道是DAG形式，则每个阶段必须以拓扑序的形式指定
- 运行时间检查：因为管道可以运行在多种数据类型上，所以不能使用编译时间检查。管道和管道模型在实际运行管道之前就会进行运行时间检查。这种检查通过数据框摘要，它描述了数据框中各列的类型
- **管道和管道模型有助于确认训练数据和测试数据经过同样的特征处理流程**


[Spark MLlib框架详解](https://www.cnblogs.com/huliangwen/p/7491797.html)

- 标准化机器学习算法的API，容易将多个算法组合成单个管道(工作流)（设计思想收sklearn启发）
- 构成组件:DataFrame、Transformer、Estimator、Pipeline、Parameter
- Pipeline内部有多个Transformer和Estimator对象
- 一系列Stage组成：
  - 若stage是Transformer类型，transform()方法将输入的DataFrame转换为另一种DataFrame；
  - 若stage是Estimator类型，先调用的fit()方法产生Transformer对象，若还有下一阶段，调用该Transformer对象的transform()方法一样会产生一个DataFrame


![Pipeline类图](https://images2017.cnblogs.com/blog/499013/201709/499013-20170907205955382-1572982213.png)