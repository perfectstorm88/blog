---
layout: post
categories: 大数据 spark
---

# 下载
通过[Spark Overview](https://spark.apache.org/docs/latest/)，进入[下载页面](https://spark.apache.org/downloads.html)

比较简单，下载到选择的版本，比如spark-2.4.5-bin-hadoop2.7.tgz，只依赖

- java 8

# Standalone部署
参见文档[Spark Standalone Mode](https://spark.apache.org/docs/latest/spark-standalone.html)

## 启动脚本 
- sbin/start-master.sh - Starts a master instance on the machine the script is executed on.
- sbin/start-slaves.sh - Starts a slave instance on each machine specified in the **conf/slaves** file.
- sbin/start-slave.sh - Starts a slave instance on the machine the script is executed on.
- sbin/start-all.sh - Starts both a master and a number of slaves as described above.
- sbin/stop-master.sh - Stops the master that was started via the sbin/start-master.sh script.
- sbin/stop-slaves.sh - Stops all slave instances on the machines specified in the **conf/slaves** file.
- sbin/stop-all.sh - Stops both the master and the slaves as described above.

**Note that these scripts must be executed on the machine you want to run the Spark master on, not your local machine.**
## 配置文件
- conf/spark-env.sh（来自conf/spark-env.sh.template） 并且拷贝此文件到到每个worker主机上
  - export HADOOP_CONF_DIR=/home/ian/installed/hadoop/etc/hadoop
  - export YARN_CONF_DIR=/home/ian/installed/hadoop/etc/hadoop
  - export SPARK_MASTER_IP=10.1.192.118
  - export SPARK_MASTER_PORT=7077
- conf/slaves 配置workder节点
# spark的端口
- 对应sparkcontext的driver的端口，默认是4040，如果被占用，会逐个加1
- 服务侦听的端口，对应./sbin/start-master.sh --port port的参数， (default: 7077 for master, random for worker)
- web UI的端口，对应./sbin/start-master.sh --webui-port PORT，(default: 8080 for master, 8081 for worker)

# 参考

- [Gitlab 重置 root 密码](https://www.cnblogs.com/kelsen/p/11167682.html)
- [gitlab修改root用户密码](https://www.cnblogs.com/rhca/p/10646641.html)