---
layout: post
categories: 大数据 spark
---

# 理解

## RDD
- RDD Persistence
  - 选择哪种存储级别？
    - 如果默认的MEMORY_ONLY能够适应的很好，不用管，这是最快
    - 如果MEMORY_ONLY不满足，选择MEMORY_ONLY_SER，更多空间高效利用率，仍然非常快(Java\Scala)
    - 不要写的磁盘上，除非数据集的计算函数很昂贵，或者能够过滤掉大量数据。否则，采用重新计算分区跟从磁盘读取数据一样快。
    - 如果你想快速的错误恢复，使用replicated存储级别。通过计算丢失的数据，所有的存储级别支持全面的容错；但是replicated级别的存储支持在RDD上继续运行task而不用等待重新计算丢失分区。
  - 移出数据
    - Spark自动监控每个数据节点的使用情况，采用LRU算法删除数据，也可以使用RDD.unpersist() 手工删除
## 共享变量
当函数在远程集群节点上执行时，它在该该函数使用的所有单独副本上工作。这些变量复制到每台机器上，而且远程机器上的变量更新不会传播回驱动程序。支持跨任务的通用读写变量是低效率的，然而，Spark提供了2种受限类型的共享变量，基于2中常用模式：广播变量和累加器。

- 广播变量:
  - 允许程序员维持一个只读变量缓存在每个机器上，而不用再task中装运拷贝。应用举例：以高效方式给每个节点一个大数据集拷贝，Spark尝试使用高效的广播算法分发广播变量，以降低通信成本。
  - Spark actions通过一系列Stage执行，这些Stages被分布的“洗牌”操作分割。

# Spark SQL Guide

## Getting Started

- 开始：SparkSession，内置了Hive特性
- 创建DataFrames，可以基于json文件创建
- 无类型DataSet操作(又名DateFrame操作）
- 访问DataFrame的列可以通过属性(df.age) 也可以通过索引(df['age'])
  - 虽然前者便于交互式数据探索，但强烈建议用户使用后者，后者是未来式，而且不会破坏列名，同时列名也是DataFrame类的属性
- 运行可编程的SQL查询：createOrReplaceTempView
- 全局临时视图：createGlobalTempView
  - Temporary只是session相关的；如果想要跨session使用要使用全局临时视图（会保持到spark应用终止），通过global_temp.xx访问
- 创建 Datasets(只要java和scala)
  - DateSets与RDDs相似，不是用java序列化，而是用专门编码器序列化对象，以用于处理和网络传输
- 与RDD的互操作： - 两种方式创建：一反射；二可编程接口
  - 通过反射推理Schema：Spark SQL可以转换一个Row对象的RDD为DataFrame，并且推理数据类型（通过对整个数据集进行采样，就像处理JSON文件一样）
    - people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))
  - 以编程方式定义Schema：如果无法提前定义kwarg的字典（例如记录结构编码为字符串，或者文本数据集被解析，并且不同字段投影不同字段），可以通过编程方式定义DataFrame，分为三步
    - 通过原始RDD创建RDD元组或列表
    - 通过StructType定义Schema
    - 把Schema应用于RDD，
    - 样例：spark.createDataFrame(people, schema)
- Aggregations内置[built-in DataFrames functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$):DataFrame操作常用的可用功能，使用此处定义的函数可提供更多的编译时安全性，以确保该函数存在。isnan（col（“ myCol”））== expr（“ isnan（myCol）”）

## Data Source
https://spark.apache.org/docs/latest/sql-data-sources.html

Spark SQL支持通过DataFrame接口对各种数据源进行操作，DataFrame可以用来做关系转换和创建临时视图。本段描述了使用Spark数据源，加载保存数据源的方案。
### 通用的Load/Save Functions
  - 默认的parquet
  - 内置json, parquet, jdbc, orc, libsvm, csv, text

```
df = spark.read.load("examples/src/main/resources/people.json", format="json")
df.select("name", "age").write.save("namesAndAges.parquet", format="parquet")
```
- Run SQL on files directly
```
df = spark.sql("SELECT * FROM parquet.`examples/src/main/resources/users.parquet`")
```

### Parquet
Parquet是Hadoop生态系统中任何项目均可使用的列式存储格式，而与数据处理框架，数据模型或编程语言的选择无关。
### ORC文件
### JSON
### JDBC To Other Databases
### Avro
### Performance Tuning
- 内存缓冲
- 其它配置项：autoBroadcastJoinThreshold
- The BROADCAST hint guides Spark to broadcast each specified table when joining them with another table or view
## Distributed SQL Engine
## PySpark Arrow

Arrow是一个内存列式数据格式，可以高效在JVM和Python进程间转换，对于使用Pandas/NumPy的用户非常有用。
调用toPandas或者createDataFrame(pandas_df)时作为一个优化器，默认是关闭的
```
spark.conf.set("spark.sql.execution.arrow.enabled", "true")
```
**即使用了Arrow，toPandas()也是把DataFrame的所有数据收集到驱动程序，并且只能在小数据集上**

Pandas UDFs user defined functions是被Spark执行，可以使用Arrow转换数据，Pandas进行数据操作：两种类型的udf
- Scalar：执行向量化的标量操作
  - 使用select和withColumn，输入和输出都是pandas.Series，长度相同
  - Spark会拆分列为多个batch，在每个batch里执行函数，再把结果联合起来
- Grouped map Pandas UDF通过groupBy().apply()实现了split-apply-combine模式，包含3步：
  - 1、通过DataFrame.groupBy拆分为分组
  - 2、在每个group里应用函数，输入输出都是 pandas.DataFrame
  - 3、聚合为一个新的DataFrame
  - 要使用groupBy().apply()用户需要定义：
    - 定义group计算的python函数
    - 定义输出Dataframe的StructType或字符串

# 样例 wordcount
- textFile
- flatMap
- map
- reductByKey
- save

# 参考

- [RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)